import os
from pathlib import Path

import numpy as np
import matplotlib.pyplot as plt

import skimage as ski
import skimage.io

import torch
from torchvision.datasets import MNIST
from torch import nn
from torch.optim import SGD
from torch.utils.data import DataLoader

MAX_EPOCHS = 8
BATCH_SIZE = 50
WEIGHT_DECAY = 1e-2

DATA_DIR = Path(__file__).parent / 'datasets' / 'MNIST'
SAVE_DIR = Path(__file__).parent / 'out_task3' / \
    'lambda_{:.3f}'.format(WEIGHT_DECAY)


def dense_to_one_hot(y, class_count):
    return np.eye(class_count)[y]


def draw_conv_filters(epoch, layer, save_dir=SAVE_DIR):
    n_columns = 8
    border = 1

    w = layer.weight.clone().detach().numpy()

    N, C, W, H = w.shape[:4]

    w_T = w.transpose(2, 3, 1, 0)
    w_T -= w_T.min()
    w_T /= w_T.max()

    n_rows = int(np.ceil(N / n_columns))

    width, height = [x * y + (x - 1) * border for x, y in zip([n_columns, n_rows],
                                                              [W, H])]
    image = np.zeros([height, width, C])

    for i in range(N):
        c = int(i % n_columns) * (W + border)
        r = int(i / n_columns) * (H + border)

        image[r: r + H, c: c + W, :] = w_T[:, :, :, i]
        filename = '%s_epoch_%02d.png' % ("conv_1",
                                          epoch)
    ski.io.imsave(os.path.join(save_dir, filename),
                  np.array(image * 255., dtype=np.uint8))


class CNN_L2(nn.Module):
    def __init__(self, in_ch_1, out_ch_1, out_ch_2, out_features_1, class_count):
        super().__init__()

        # 1st convolution layer
        self.conv_1 = nn.Conv2d(in_channels=in_ch_1,
                                out_channels=out_ch_1,
                                kernel_size=5,
                                stride=1, padding=2, bias=True)
        self.maxpool_1 = nn.MaxPool2d(kernel_size=2, stride=2)

        # 2nd convolution layer
        self.conv_2 = nn.Conv2d(in_channels=out_ch_1,
                                out_channels=out_ch_2,
                                kernel_size=5,
                                stride=1, padding=2, bias=True)
        self.maxpool_2 = nn.MaxPool2d(kernel_size=2, stride=2)

        # FC layers
        self.fc_1 = nn.Linear(in_features=out_ch_2 * 7 * 7,
                              out_features=out_features_1,
                              bias=True)
        self.fc_logits = nn.Linear(in_features=out_features_1,
                                   out_features=class_count,
                                   bias=True)

        self.reset_parameters()

    def reset_parameters(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(
                    m.weight, mode='fan_in', nonlinearity='relu')
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear) and m is not self.fc_logits:
                nn.init.kaiming_normal_(
                    m.weight, mode='fan_in', nonlinearity='relu')
                nn.init.constant_(m.bias, 0)
        self.fc_logits.reset_parameters()

    def forward(self, x):
        x = x.float()

        # 1st convolution layer pass
        h_conv1 = self.conv_1(x)
        h_conv1 = self.maxpool_1(h_conv1)
        h_conv1 = torch.relu(h_conv1)
        # mo≈æe i h_conv1.relu() ili nn.functional.relu(h_conv1)

        # 2nd convolution layer pass
        h_conv2 = self.conv_2(h_conv1)
        h_conv2 = self.maxpool_2(h_conv2)
        h_conv2 = torch.relu(h_conv2)

        # Flatten output to 1D
        h_conv = h_conv2.view(h_conv2.shape[0], -1)

        # 1st FC layer pass
        h_fc = self.fc_1(h_conv)
        h_fc = torch.relu(h_fc)

        # 2nd FC layer pass
        logits = self.fc_logits(h_fc)
        return logits

    def predict(self, x):
        return torch.argmax(self.forward(x))

    @staticmethod
    def loss(x, y):
        sumlog = torch.log(torch.sum(torch.exp(x), dim=1))
        mulsum = torch.sum(x * y, dim=1)

        return torch.mean(sumlog - mulsum)

    def get_val_losses(self, x, y):
        return val_losses.append(float(self.loss(self.forward(x).clone().detach(), y)))

    def get_val_losses(self, x, y):
        return float(self.loss(self.forward(x).clone().detach(), y))

    def train(self, x, y, x_valid, y_valid, regularization_factor):
        x = torch.tensor(x)
        y = torch.tensor(y)
        x_valid = torch.tensor(x_valid)
        y_valid = torch.tensor(y_valid)

        optimizer = SGD([
            {"params": [*self.conv_1.parameters(),
                        *self.conv_2.parameters(),
                        *self.fc_1.parameters(),
                        *self.fc_logits.parameters()], "weight_decay": regularization_factor}], lr=1e-2)

        draw_conv_filters(0, self.conv_1)

        losses = list()
        validation_losses = list()
        for i in range(1, MAX_EPOCHS+1):
            print("Epoch: {}".format(i))

            # Prepare mini batches
            x_batches = torch.split(x, BATCH_SIZE)
            y_batches = torch.split(y, BATCH_SIZE)

            batches_loss = list()
            for (x_batch, y_batch) in zip(x_batches, y_batches):
                loss = self.loss(self.forward(x_batch), y_batch)
                print("Loss: {}".format(float(loss)))

                # Save loss
                batches_loss.append(float(loss))

                loss.backward()
                optimizer.step()
                optimizer.zero_grad()

            losses.append(np.mean(batches_loss))
            validation_losses.append(self.get_val_losses(x_valid, y_valid))

            draw_conv_filters(i, self.conv_1)

        epochs = [i for i in range(MAX_EPOCHS)]
        plt.plot(epochs,
                 losses,
                 label="train losses")
        plt.plot(epochs,
                 validation_losses,
                 label="validation losses")
        plt.legend(loc="best")
        plt.show()


if __name__ == "__main__":
    # Define CNN model
    model = CNN_L2(in_ch_1=1,
                   out_ch_1=16,
                   out_ch_2=32,
                   out_features_1=512,
                   class_count=10)

    # Prepare MNIST dataset
    ds_train, ds_test = MNIST(DATA_DIR, train=True,
                              download=True), MNIST(DATA_DIR, train=False)
    train_x = ds_train.data.reshape(
        [-1, 1, 28, 28]).numpy().astype(float) / 255
    train_y = ds_train.targets.numpy()
    train_x, valid_x = train_x[:55000], train_x[55000:]
    train_y, valid_y = train_y[:55000], train_y[55000:]
    test_x = ds_test.data.reshape(
        [-1, 1, 28, 28]).numpy().astype(float) / 255
    test_y = ds_test.targets.numpy()
    train_mean = train_x.mean()
    train_x, valid_x, test_x = (
        x - train_mean for x in (train_x, valid_x, test_x))
    train_y, valid_y, test_y = (dense_to_one_hot(y, 10)
                                for y in (train_y, valid_y, test_y))

    # Train model
    model.train(train_x,
                train_y,
                valid_x,
                valid_y,
                WEIGHT_DECAY)
